import os
import json
import pysocrata
import numpy as np
from tqdm import tqdm


# import datafy
import sys; sys.path.insert(0, "../../limited-requests")
import limited_requests

DOMAIN = "data.cityofnewyork.us"
FILE_SLUG = "nyc"

# First check to see whether or not a geospatial.json file already exists. We won't recreate the file if it already
# exists. This means that:
# 1. To regenerate the information from scratch, the file must first be deleted.
# 2. Manual edits to the glossary will be preserved (this is a behavior that we want) and considered by the second part
#    of this script.
preexisting = os.path.isfile("../../../data/" + FILE_SLUG + "/glossaries/geospatial.json")


# If not, build out an initial list.
if not preexisting:
    # Obtain NYC open data portal credentials.
    with open("../../../auth/nyc-open-data.json", "r") as f:
        nyc_auth = json.load(f)

    # Use pysocrata to fetch portal metadata.
    nyc_datasets = pysocrata.get_datasets(**nyc_auth)
    nyc_datasets = [d for d in nyc_datasets if d['resource']['type'] != 'story']  # stories excluded manually

    # Get geospatial datasets.
    nyc_types = [d['resource']['type'] for d in nyc_datasets]
    volcab_map = {'dataset': 'table', 'href': 'link', 'map': 'geospatial dataset', 'file': 'blob'}
    nyc_types = list(map(lambda d: volcab_map[d], nyc_types))
    nyc_endpoints = [d['resource']['id'] for d in nyc_datasets]
    geospatial_indices = np.nonzero([t == 'geospatial dataset' for t in nyc_types])
    geospatial_endpoints = np.array(nyc_endpoints)[geospatial_indices]
    geospatial_datasets = np.array(nyc_datasets)[geospatial_indices]

    # Build the data representation.
    datasets = []
    for dataset in geospatial_datasets:
        endpoint = dataset['resource']['id']
        slug = "https://" + DOMAIN + "/api/geospatial/" + endpoint + "?method=export&format=GeoJSON"
        datasets.append(
            {
                'endpoint': endpoint,
                'resource': slug,
                'dataset': '.',
                'type': 'geojson',
                'rows': '?',
                'columns': '?',
                'filesize': '?',
                'flags': ''
             }
        )

    # Write to the file.
    with open("../../../data/" + FILE_SLUG + "/glossaries/geospatial.json", "w") as fp:
        json.dump(datasets, fp, indent=4)

    del datasets


# At this point we know that the file exists. But its contents may not contain the row and column size information that
# we need, because if it was just regenerated by the loop above that stuff will have been populated simply with "?" so
# far.

# Begin by loading in the data that we have.
with open("../../../data/" + FILE_SLUG + "/glossaries/geospatial.json", "r") as fp:
    datasets = json.loads(fp.read())


# Get the datasets we need to extract things from.
datasets_needing_extraction = [d for d in datasets\
                               if (d['filesize'] == "?")]
indices = [i for i, d in enumerate(datasets)\
           if (d['filesize'] == "?")]
uris = [d['resource'] for d in datasets_needing_extraction]
endpoints = [d['endpoint'] for d in datasets_needing_extraction]


# Create a q for managing jobs.
q = limited_requests.q()
timeout = 60


# Run the process.
try:
    for dataset, i, uri in tqdm(zip(datasets_needing_extraction, indices, uris)):
        sizing = limited_requests.limited_get(uri, q, timeout=timeout)
        if sizing:  # If successful.
            assert len(sizing) == 1  # the geospatial datasets should be returned as singular objects
            datasets[i]['rows'] = int(sizing[0]['rows'])
            datasets[i]['columns'] = int(sizing[0]['columns'])
            datasets[i]['filesize'] = int(sizing[0]['filesize'])
        else:  # If not successful.
            datasets[i]['filesize'] = ">{0}s".format(timeout)
except Exception as e:
    import pdb; pdb.set_trace()
    print("HELLO")
finally:
    # Whether we succeeded or got caught on a fatal error, in either case save the output to file before exiting.
    with open("../../../data/" + FILE_SLUG + "/glossaries/geospatial.json", "w") as fp:
        json.dump(datasets, fp, indent=4)