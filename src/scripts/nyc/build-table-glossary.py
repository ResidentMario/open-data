import os
import json
import pysocrata
import numpy as np
from tqdm import tqdm

import sys; sys.path.insert(0, "../../endpoint-pager/")
import pager

DOMAIN = "data.cityofnewyork.us"
FILE_SLUG = "nyc"

# First check to see whether or not a tables_backup.json file already exists. We won't recreate the file if it already
# exists. This means that:
# 1. To regenerate the information from scratch, the file must first be deleted.
# 2. Manual edits to the glossary will be preserved (this is a behavior that we want) and considered by the second part
#    of this script.
preexisting = os.path.isfile("../../../data/" + FILE_SLUG + "/glossaries/tables.json")


# If not, build out an initial list.
if not preexisting:
    # Obtain NYC open data portal credentials.
    with open("../../../auth/nyc-open-data.json", "r") as f:
        nyc_auth = json.load(f)

    # Use pysocrata to fetch portal metadata.
    nyc_datasets = pysocrata.get_datasets(**nyc_auth)
    nyc_datasets = [d for d in nyc_datasets if d['resource']['type'] != 'story']  # stories excluded manually

    # Get table datasets.
    nyc_types = [d['resource']['type'] for d in nyc_datasets]
    volcab_map = {'dataset': 'table', 'href': 'link', 'map': 'geospatial dataset', 'file': 'blob'}
    nyc_types = list(map(lambda d: volcab_map[d], nyc_types))
    nyc_endpoints = [d['resource']['id'] for d in nyc_datasets]
    table_indices = np.nonzero([t == 'table' for t in nyc_types])
    table_endpoints = np.array(nyc_endpoints)[table_indices]
    table_datasets = np.array(nyc_datasets)[table_indices]

    # Build the data representation.
    nyc_table_datasets = []
    for dataset in table_datasets:
        endpoint = dataset['resource']['id']
        slug = "https://" + DOMAIN + "/api/views/" + endpoint + "/rows.csv?accessType=DOWNLOAD"
        nyc_table_datasets.append(
            {
                'endpoint': endpoint,
                'resource': slug,
                'dataset': '.',
                'type': 'csv',
                'rows': '?',
                'columns': '?',
                'filesize': '?',
                'flags': ''
             }
        )

    # Write to the file.
    with open("../../../data/" + FILE_SLUG + "/glossaries/tables.json", "w") as fp:
        json.dump(nyc_table_datasets, fp, indent=4)

    del nyc_table_datasets


# At this point we know that the file exists. But its contents may not contain the row and column size information that
# we need, because if it was just regenerated by the loop above that stuff will have been populated simply with "?" so
# far.

# Begin by loading in the data that we have.
with open("../../../data/" + FILE_SLUG + "/glossaries/tables.json", "r") as fp:
    nyc_table_datasets = json.loads(fp.read())

# Gather additional data by scraping from Socrata whenever needed.
try:
    for dataset in tqdm(nyc_table_datasets):
            if (dataset['rows'] == "?") or (dataset['columns'] == "?"):

                # Catch an error where the dataset has been deleted, warn but continue.
                try:
                    rowcol = pager.page_socrata(DOMAIN, dataset['endpoint'], timeout=10)
                except pager.DeletedEndpointException:
                    print("WARNING: the '{0}' endpoint appears to have been removed.".format(dataset['endpoint']))
                    dataset['flags'] = 'removed'
                    continue

                # If no repairable errors were caught, write in the information.
                # (if a non-repairable error was caught the data gets sent to the outer finally block)
                dataset['rows'], dataset['columns'] = rowcol['Rows'], rowcol['Columns']
            else:
                continue
finally:
    # Whether we succeeded or got caught on a fatal error, in either case save the output to file before exiting.
    with open("../../../data/" + FILE_SLUG + "/glossaries/tables.json", "w") as fp:
        json.dump(nyc_table_datasets, fp, indent=4)
