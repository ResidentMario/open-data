# import os
# import json
# import pysocrata
# import numpy as np
# from tqdm import tqdm
#
#
# # import datafy
# import sys; sys.path.insert(0, "../../limited-requests")
# import limited_requests
#
# DOMAIN = "data.cityofnewyork.us"
# FILE_SLUG = "nyc"
#
# # First check to see whether or not a blob-resources.json file already exists. We won't recreate the file if it already
# # exists. This means that:
# # 1. To regenerate the information from scratch, the file must first be deleted.
# # 2. Manual edits to the glossary will be preserved (this is a behavior that we want) and considered by the second part
# #    of this script.
# preexisting = os.path.isfile("../../../data/" + FILE_SLUG + "/glossaries/blob-resources.json")
#
#
# # If not, build out an initial list.
# if not preexisting:
#     # Obtain NYC open data portal credentials.
#     with open("../../../auth/nyc-open-data.json", "r") as f:
#         nyc_auth = json.load(f)
#
#     # Use pysocrata to fetch portal metadata.
#     nyc_datasets = pysocrata.get_datasets(**nyc_auth)
#     nyc_datasets = [d for d in nyc_datasets if d['resource']['type'] != 'story']  # stories excluded manually
#
#     # Get blob resources.
#     nyc_types = [d['resource']['type'] for d in nyc_datasets]
#     volcab_map = {'dataset': 'table', 'href': 'link', 'map': 'geospatial dataset', 'file': 'blob'}
#     nyc_types = list(map(lambda d: volcab_map[d], nyc_types))
#     nyc_endpoints = [d['resource']['id'] for d in nyc_datasets]
#     indices = np.nonzero([t == 'blob' for t in nyc_types])
#     endpoints = np.array(nyc_endpoints)[indices]
#     resources = np.array(nyc_datasets)[indices]
#
#     # Build the data representation.
#     resources = []
#     for resource in resources:
#         endpoint = resource['resource']['id']
#         slug = "https://" + DOMAIN + "/api/geospatial/" + endpoint + "?method=export&format=GeoJSON"
#         resources.append(
#             {
#                 'endpoint': endpoint,
#                 'resource': slug,
#                 'flags': ''
#             }
#         )
#
#     # Write to the file.
#     with open("../../../data/" + FILE_SLUG + "/glossaries/blob-resources.json", "w") as fp:
#         json.dump(resources, fp, indent=4)
#
#     del resources
#
#
# # At this point we know that the file exists. But its contents may not contain the row and column size information that
# # we need, because if it was just regenerated by the loop above that stuff will have been populated simply with "?" so
# # far.
#
# # Begin by loading in the resource representations.
# with open("../../../data/" + FILE_SLUG + "/glossaries/blob-resources.json", "r") as fp:
#     resources = json.loads(fp.read())
#
# # If a daset representation exists, load that too.
#
#
#
# # Get the datasets we need to extract things from.
# resources_needing_extraction = [d for d in resources if (d['filesize'] == "?")]
# indices = [i for i, d in enumerate(resources) if (d['filesize'] == "?")]
# uris = [d['resource'] for d in resources_needing_extraction]
# # endpoints = [d['endpoint'] for d in resources_needing_extraction]
#
#
# # Create a q for managing jobs.
# q = limited_requests.q()
# timeout = 60
#
#
# # Run the process.
# try:
#     for dataset, i, uri in tqdm(zip(datasets_needing_extraction, indices, uris)):
#         sizing = limited_requests.limited_get(uri, q, timeout=timeout)
#         if sizing:  # If successful.
#             assert len(sizing) == 1  # the geospatial datasets should be returned as singular objects
#             datasets[i]['rows'] = int(sizing[0]['rows'])
#             datasets[i]['columns'] = int(sizing[0]['columns'])
#             datasets[i]['filesize'] = int(sizing[0]['filesize'])
#         else:  # If not successful.
#             datasets[i]['filesize'] = ">{0}s".format(timeout)
# except Exception as e:
#     import pdb; pdb.set_trace()
#     print("HELLO")
# finally:
#     # Whether we succeeded or got caught on a fatal error, in either case save the output to file before exiting.
#     with open("../../../data/" + FILE_SLUG + "/glossaries/geospatial.json", "w") as fp:
#         json.dump(datasets, fp, indent=4)