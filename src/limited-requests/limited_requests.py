"""
This is a small module which defines an API for downloading resources from the web within a time-limited context. In
other words, while `requests.get` allows you to download something off the web, `limited_requests.limited_get` allows
you to download it off the web, but only if it does so successfully within <timeout> seconds (and, if the file provides
content-length information in its header, the file itself is of <sizeout> size).
"""

import requests
import multiprocessing as mp
import sys


sys.path.append("../../")
import datafy


class FileTooLargeException(Exception):
    """
    This exception is meant to be thrown when sizeout is specified, the URI has a content-length header, and the
    content-length header specifies a filesize larger than sizeout.
    """
    pass


def _fetch(uri, q, reducer, sizeout=None):

    if sizeout:
        r = requests.head(uri)
        if 'content-length' in r.headers:
            if 'content-length' > sizeout:
                raise FileTooLargeException

    dataset_tuples = datafy.get(uri)
    print(dataset_tuples)  # for debugging
    q.put(reducer(dataset_tuples))


def _size_up(dataset_tuples):
    dataset_representations = []
    for data, fp, type in dataset_tuples:

        # How we "size up" the dataset depends on what format it is provided in.

        # If it's provided in a tabular format, e.g. as a parsed tabular or geospatial dataset, we are given a
        # DataFrame or GeoDataFrame, and can analyze it as such. We'll duck type this because this might not always
        # work; it fails in particular fairly often in the case of Excel files.
        try:
            dataset_representations.append({
                'columns': len(data.columns),
                'rows': len(data),
                'filesize': sys.getsizeof(data),  # this was formerly `data.memory_usage().sum()`
                'type': type,
                'dataset': fp
            })

        # Otherwise, do a basic sizing and print the type to console (WIP).
        except (AttributeError, ValueError):
            dataset_representations.append({
                'columns': -1,  # -1 is a signal value, used because it gets converted to an int upstream
                'rows': -1,
                'filesize': sys.getsizeof(data),
                'type': type,
                'dataset': fp
            })

    return dataset_representations


def q():
    return mp.Queue()


def limited_get(uri, q, reducer=_size_up, timeout=60, sizeout=None):
    """
    Implemented a timed request. Note: this function blocks.

    Parameters
    ----------
    uri: str
        The resource URI.
    q: mp.Queue
        An `mp.Queue` object for message passing. For ease of use initialzie using `q = limited_requests.q()`.
    reducer: func
        A function to be run after a resource is successfully downloaded. When this occurs, the resource is returned in
        the format generated by the `datafy.get` function call: [(<data>, <type string>), ...], a list of tuples. The
        reducer should input this representation and return what you want to get out of the data (probably a list of
        "other stuff"). The default reducer is `_size_up`, which returns filesize information.
    timeout: int
        The maximum amount of time that this entire process will get. If the process takes longer than this, a SIGINT
        will be raised to interrupt and kill the process and move on. This, the crux of the whole problem addressed
        by this module, is done in order to avoid getting stuck on inordinately large files (for which sizeout can't
        be specified).
    sizeout: int, default None
        The maximum size. Note that this parameter will only work for resources which define a `content-length` header.

    Returns
    -------
    Whatever you get by reducing the URI, assuming the job completes. None, if the job doesn't complete.
    """
    p = mp.Process(target=_fetch, args=(uri, q), kwargs={'reducer': reducer, 'sizeout': sizeout})
    p.start()
    p.join(timeout)
    p.terminate()
    # If the process succeeded the exitcode is 0.
    if p.exitcode == 0:
        repr = q.get()
        for dataset in repr:
            dataset['resource'] = uri
        return repr
